{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ca79d3-e6eb-4ade-ae8a-2c3f8921604a",
   "metadata": {},
   "source": [
    "[Getting Started With Statsbomb Data](https://znstrider.github.io/2018-11-11-Getting-Started-with-StatsBomb-Data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f42ec1-f444-44cd-82c8-98d7f42962ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import json_normalize\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm.notebook import tqdm  # Import the notebook version of tqdm\n",
    "\n",
    "'''\n",
    "Set mypath to your open-data-master/data/ path\n",
    "'''\n",
    "mypath = 'StatsBomb/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b222cb-94f4-4558-b659-bd7e6ba7409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVENTS AND FREEZE-FRAMES\n",
    "files = [f for f in listdir(mypath+'events/') if isfile(join(mypath+'events/', f))]\n",
    "try: #if you're on MacOS like I am this file might mess with you, so try removing it\n",
    "    files.remove('.DS_Store')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "dfs = {}\n",
    "ffs = {}\n",
    "\n",
    "for file in files:\n",
    "    with open(mypath+'events/'+file) as data_file:\n",
    "        #print (mypath+'events/'+file)\n",
    "        data = json.load(data_file)\n",
    "        #get the nested structure into a dataframe\n",
    "        df = json_normalize(data, sep = \"_\").assign(match_id = file[:-5])\n",
    "        #store the dataframe in a dictionary with the match id as key (remove '.json' from string)\n",
    "        dfs[file[:-5]] = df.set_index('id')    \n",
    "        shots = df.loc[df['type_name'] == 'Shot'].set_index('id')\n",
    "\n",
    "        #get the freeze frame information for every shot in the df\n",
    "        for id_, row in shots.iterrows():\n",
    "            try:\n",
    "                ff = json_normalize(row.shot_freeze_frame, sep = \"_\")\n",
    "                ff = ff.assign(x = ff.apply(lambda x: x.location[0], axis = 1)).\\\n",
    "                        assign(y = ff.apply(lambda x: x.location[1], axis = 1)).\\\n",
    "                        drop('location', axis = 1).\\\n",
    "                        assign(id = id_)\n",
    "                ffs[id_] = ff\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "#concatenate all the dictionaries\n",
    "#this creates a multi-index with the dictionary key as first level\n",
    "df = pd.concat(dfs, axis = 0)\n",
    "\n",
    "#split locations into x and y components\n",
    "df[['location_x', 'location_y']] = df['location'].apply(pd.Series)\n",
    "df[['pass_end_location_x', 'pass_end_location_y']] = df['pass_end_location'].apply(pd.Series)\n",
    "\n",
    "#split the shot_end_locations into x,y and z components (some don't include the z-part)\n",
    "df['shot_end_location_x'], df['shot_end_location_y'], df['shot_end_location_z'] = np.nan, np.nan, np.nan\n",
    "end_locations = np.vstack(df.loc[df.type_name == 'Shot'].shot_end_location.apply(lambda x: x if len(x) == 3\n",
    "                                       else x + [np.nan]).values)\n",
    "df.loc[df.type_name == 'Shot', 'shot_end_location_x'] = end_locations[:, 0]\n",
    "df.loc[df.type_name == 'Shot', 'shot_end_location_y'] = end_locations[:, 1]\n",
    "df.loc[df.type_name == 'Shot', 'shot_end_location_z'] = end_locations[:, 2]\n",
    "events_df = df.drop(['location', 'pass_end_location', 'shot_end_location'], axis = 1)\n",
    "\n",
    "#concatenate all the Freeze Frame dataframes\n",
    "ff_df = pd.concat(ffs, axis = 0)\n",
    "\n",
    "\n",
    "# MATCHES\n",
    "files = [f for f in listdir(mypath+'matches/') if isfile(join(mypath+'matches/', f))]\n",
    "try:\n",
    "    files.remove('.DS_Store')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "matches_dfs = {}\n",
    "for file in files:\n",
    "    with open(mypath+'matches/'+file) as data_file:\n",
    "        #print (mypath+'lineups/'+file)\n",
    "        data = json.load(data_file)\n",
    "        #get the nested structure into a dataframe\n",
    "        df_ = json_normalize(data, sep = \"_\")\n",
    "        #store the dataframe in a dictionary with the competition id as key\n",
    "        matches_dfs[file[:-5]] = df_\n",
    "\n",
    "matches_df = pd.concat(matches_dfs)\n",
    "\n",
    "\n",
    "# LINEUPS w Minutes played\n",
    "files = [f for f in listdir(mypath+'lineups/') if isfile(join(mypath+'lineups/', f))]\n",
    "try: #if you're on MacOS like I am this file might mess with you, so try removing it\n",
    "    files.remove('.DS_Store')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "dfs = {}\n",
    "ffs = {}\n",
    "\n",
    "for file in files:\n",
    "    with open(mypath+'lineups/'+file) as data_file:\n",
    "        #print (mypath+'events/'+file)\n",
    "        data = json.load(data_file)\n",
    "        #get the nested structure into a dataframe\n",
    "        df = json_normalize(data, sep = \"_\").assign(match_id = file[:-5])\n",
    "        df_1 = json_normalize(df.lineup.iloc[0], sep = \"_\").assign(\n",
    "                team_id = df.team_id.iloc[0],\n",
    "                team_name = df.team_name.iloc[0],\n",
    "                match_id = df.match_id.iloc[0])\n",
    "        df_2 = json_normalize(df.lineup.iloc[1], sep = \"_\").assign(\n",
    "                team_id = df.team_id.iloc[1],\n",
    "                team_name = df.team_name.iloc[1],\n",
    "                match_id = df.match_id.iloc[1])\n",
    "        dfs[file[:-5]] = pd.concat([df_1, df_2])\n",
    "\n",
    "lineups_df = pd.concat(dfs.values())\n",
    "\n",
    "# get the lengths of matches\n",
    "match_lengths = events_df.groupby('match_id')['minute'].max()\n",
    "\n",
    "# get all substitutions\n",
    "substitutions = events_df.loc[events_df.substitution_outcome_name.notnull(),\n",
    "                               ['minute', 'player_name', 'substitution_replacement_name']].\\\n",
    "                    reset_index().\\\n",
    "                    drop('id', axis = 1).\\\n",
    "                    rename(columns = {'level_0': 'match_id'}).\\\n",
    "                    set_index(['match_id'])\n",
    "\n",
    "# assign all minutes played to the lineups_df\n",
    "a = lineups_df.reset_index().set_index('match_id').assign(minutes_played = match_lengths)\n",
    "\n",
    "for idx, row in substitutions.iterrows():\n",
    "    a.loc[(a.index == idx)&(a.player_name == row.player_name), 'minutes_played'] = row.minute\n",
    "    a.loc[(a.index == idx)&(a.player_name == row.substitution_replacement_name), 'minutes_played'] = \\\n",
    "        a.loc[(a.index == idx)&(a.player_name == row.substitution_replacement_name), 'minutes_played'] - row.minute\n",
    "\n",
    "lineups_df = a.reset_index().set_index(['match_id', 'index'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
